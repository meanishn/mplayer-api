const path = require('path');
const fs = require('fs');
const _ = require('lodash');
const sequelize = require('sequelize');

const models = require('./app/models');

const Op = sequelize.Op;

const Artist = models.artist;
const Track = models.track;
const Tag = models.tag;
const file = path.normalize(path.join(__dirname, '/data/tracks/data.json'));
let dataObj;

// const upsertCompiled = template(`
// <%= bulkQuery %>
// ON CONFLICT (<%= constraint %>) DO UPDATE
// SET
// <%= sets %>
// ;`);

function upsertCompiled(bulkQuery, constraint, sets) {
  return `${bulkQuery} ON CONFLICT (${constraint}) DO UPDATE SET ${sets} RETURNING *`;
}

/**
 * Postgres bulk Upsert
 * Hacked from the insert query generated by Sequelize
 * https://www.postgresql.org/docs/10/static/sql-insert.html
 *
 * @param {Sequelize.Model} Model - Model to do bulk upsert on
 * @param {any[]} values - bulk values for model
 * @param {string} constraint - constraint name or primary key to check
 * @param {string[]} fieldsToSet - fields to set on conflict
 *
 */
async function bulkUpsert(Model,
  values,
  constraint,
  fieldsToSet) {
  const model = Model;

  // Map attributes to fields for serial identification
  const fieldMappedAttributes = {};
  for (const attr of Object.keys(model.tableAttributes)) {
    fieldMappedAttributes[model.rawAttributes[attr].field || attr] = model.rawAttributes[attr];
  }

  const queryGererator = models.sequelize.getQueryInterface().QueryGenerator;
  let bulkQuery = queryGererator.bulkInsertQuery(
    Model.getTableName(),
    values,
    {},
    fieldMappedAttributes,
  );

  bulkQuery = bulkQuery.replace(/;\s*$/, '');

  const sets = fieldsToSet.map(f => ` ${f} = EXCLUDED.${f} `).join(',');
  const bulkUpsertQuery = upsertCompiled(bulkQuery, constraint, sets);
  await models.sequelize.query(bulkUpsertQuery);
}

function parse(filePath) {
  return new Promise((resolve, reject) => {
    fs.readFile(filePath, 'utf8', (err, content) => {
      if (err) {
        return reject(err);
      }
      const data = JSON.parse(content);
      return resolve(data);
    });
  });
}

function extractTags(str) {
  const tags = [];
  const regex = /\[([^\]]+)\]/gm;
  let match = regex.exec(str);
  while (match !== null) {
    if (match[1]) {
      tags.push(match[1]);
    }
    match = regex.exec(str);
  }
  return tags;
}

async function generateModelObject() {
  const data = await parse(file);
  const transformed = {};
  data.forEach((d) => {
    const tags = extractTags(d.title);
    const title = d.title.replace(/\[([^\]]+)\]/g, '').trim();
    const id = d.downloadURL.match(/https:\/\/soundgasm.net\/sounds\/(.*[^.m4a])/)[1];
    const artist = transformed[d.uploader] = transformed[d.uploader] || {};
    const tracks = artist.tracks = artist.tracks || [];
    artist.name = d.uploader;
    artist.sources = d.sources;
    tracks.push({
      unique_id: id,
      url: d.downloadURL,
      title,
      tags,
    });
  });

  dataObj = transformed;
  return transformed;
}
function getAllArtists() {
  return Object.keys(dataObj).map(key => dataObj[key]);
}

function getAllTracks() {
  return Object.keys(dataObj).map(key => dataObj[key].tracks);
}

function getTracks(artist) {
  return dataObj[artist].tracks;
}
async function setTracks(artists) {
  const p = [];
  let tags = [];
  artists.forEach((artist) => {
    const tracks = getTracks(artist.name);
    // p.push(Track.bulkCreate(tracks.map((track) => {
    //   track.artist_id = artist.id;
    //   delete track.uploader;
    //   return track;
    // })));
    p.push(bulkUpsert(Track, tracks.map((track) => {
      const cloned = _.clone(track);
      cloned.artist_id = artist.id;
      cloned.createdAt = new Date();
      cloned.updatedAt = new Date();
      tags = tags.concat(track.tags);
      delete cloned.tags;
      delete cloned.uploader;
      return cloned;
    }), 'unique_id', ['title']));
  });
  const uniqueTags = new Set(tags);
  await bulkUpsert(Tag, [...uniqueTags].map(tag => ({ name: tag, createdAt: new Date(), updatedAt: new Date() })), 'name', ['name']);
  // safely ignore duplicate error, this is to avoid checking for existing
  // track. If duplicate tracks are found (which is quite the case) simply fail and ignore the error
  return Promise.all(p.map(promise => promise.catch(e => e)));
}

async function createArtistWithTracks(artists) {
  const p = [];
  artists.forEach((artist) => {
    const tracks = getTracks(artist.name);
    const publicProfiles = artist.sources ? JSON.stringify(artist.sources) : '';
    p.push(Artist.create({
      name: artist.name,
      public_profiles: publicProfiles,
      tracks,
    }, {
      include: [Track],
    },
    ));
  });
  await Promise.all(p);
}

async function seedArtist() {
  const artists = getAllArtists();
  const artistNames = artists.map(artist => artist.name);
  const existingArtists = await Artist.findAll({
    where: {
      name: {
        [Op.in]: artistNames,
      },
    },
  });
  const newArtists = artists.filter(artist => !existingArtists.find(a => a.name === artist.name));
  const x = await setTracks(existingArtists);
  const y = await createArtistWithTracks(newArtists);
  console.log('Artist and track Seed Complete');

  const allTracks = _.flatten(getAllTracks());
  setTags(allTracks);
  console.log(dataObj);
  console.log(allTracks);
}

async function setTags(tracks) {
  const p = [];
  for (let track of tracks) {
    track = track;
    const tags = track.tags;
    const dbTags = await Tag.findAll({
      where: {
        name: {
          [Op.in]: tags,
        },
      },
    });
    const dbTrack = await Track.findOne({
      where: {
        unique_id: track.unique_id,
      },
    });
    await dbTrack.setTags(dbTags);
  }
}

generateModelObject()
.then(seedArtist)
.then(() => console.log('DONE'))
.catch((e) => {
  console.error(e);
});

